{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd354e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ffe49",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab8631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/chest_xray/test'\n",
    "train_path = 'data/chest_xray/train'\n",
    "val_path = 'data/chest_xray/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b597412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_info = pd.read_csv('./img_info.csv')\n",
    "test_df = img_info.loc[img_info.loc[:, 'data'] == 'Test']\n",
    "train_df = img_info.loc[img_info.loc[:, 'data'] == 'Train']\n",
    "val_df = test_df = img_info.loc[img_info.loc[:, 'data'] == 'Validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d71c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Normal Images in the Training Data: 0.5093694606229425\n",
      "Percentage of Pneumonia Images in the Training Data: 0.4906305393770575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/qwgd8hzn23n90l6twpb5dty40000gn/T/ipykernel_42724/907957275.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  normal_images.loc[:,'imbalance'] = 1\n"
     ]
    }
   ],
   "source": [
    "normal_images = train_df.loc[train_df.loc[:,'label'] == 0]\n",
    "normal_images.loc[:,'imbalance'] = 1\n",
    "train_df = pd.concat([train_df, normal_images, normal_images]).reset_index(drop=True)\n",
    "print('Percentage of Normal Images in the Training Data: {}'.format(\n",
    "    len(train_df.loc[train_df.loc[:,'label'] == 0])/len(train_df.loc[:,'label'])))\n",
    "print('Percentage of Pneumonia Images in the Training Data: {}'.format(\n",
    "    len(train_df.loc[train_df.loc[:,'label'] == 1])/len(train_df.loc[:,'label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4709732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, img_dir_path, transforms=None):\n",
    "        \"\"\"\n",
    "        You can set your custom dataset to take in more parameters than specified\n",
    "        here. But, I recommend at least you start with the three I listed here,\n",
    "        as these are standard\n",
    "\n",
    "        csv_file (str): file path to the csv file you created /\n",
    "        df (pandas df): pandas dataframe\n",
    "\n",
    "        img_dir_path: directory path to your images\n",
    "        transform: Compose (a PyTorch Class) that strings together several\n",
    "          transform functions (e.g. data augmentation steps)\n",
    "\n",
    "        One thing to note -- you technically could implement `transform` within\n",
    "        the dataset. No one is going to stop you, but you can think of the\n",
    "        transformations/augmentations you do as a hyperparameter. If you treat\n",
    "        it as a hyperparameter, you want to be able to experiment with different\n",
    "        transformations, and therefore, it would make more sense to decide those\n",
    "        transformations outside the dataset class and pass it to the dataset!\n",
    "        \"\"\"\n",
    "        self.img_labels = df\n",
    "        self.img_dir = img_dir_path\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns: (int) length of your dataset\n",
    "        \"\"\"\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and returns your sample (the image and the label) at the\n",
    "        specified index\n",
    "\n",
    "        Parameter: idx (int): index of interest\n",
    "\n",
    "        Returns: image, label\n",
    "        \"\"\"\n",
    "\n",
    "        img_path =  self.img_labels.iloc[idx, 1]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        #image = read_image(img_path)\n",
    "\n",
    "        label = self.img_labels.iloc[idx, -1]\n",
    "        \n",
    "        imbalance = self.img_labels.iloc[idx, -2]\n",
    "\n",
    "        if self.transforms:\n",
    "            \n",
    "            if imbalance and not label:\n",
    "                image = transforms(image)\n",
    "                image = imbalance_transform(image)\n",
    "                \n",
    "            else:\n",
    "                image = transforms(image)\n",
    "               \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834e7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose(\n",
    "    [\n",
    "        T.Resize((224,224), antialias=None, interpolation=InterpolationMode.BICUBIC),\n",
    "        T.RandomApply([\n",
    "            T.GaussianBlur(kernel_size=(5,5), sigma=(0.1, 0.2))\n",
    "        ], p=0.5),\n",
    "        T.RandomEqualize(),\n",
    "        T.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "imbalance_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(degrees=10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    T.RandomErasing(p=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d00156bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomImageDataset(train_df, train_path, transforms=transforms)\n",
    "val_data = CustomImageDataset(val_df, val_path, transforms=transforms)\n",
    "test_data = CustomImageDataset(test_df, test_path, transforms=transforms)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae732e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0784, 0.0941, 0.0941,  ..., 0.0902, 0.0980, 0.0275],\n",
       "          [0.0784, 0.0941, 0.0941,  ..., 0.0941, 0.0941, 0.0275],\n",
       "          [0.0745, 0.0941, 0.0980,  ..., 0.0941, 0.0941, 0.0275],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0118, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0784, 0.0941, 0.0941,  ..., 0.0902, 0.0980, 0.0275],\n",
       "          [0.0784, 0.0941, 0.0941,  ..., 0.0941, 0.0941, 0.0275],\n",
       "          [0.0745, 0.0941, 0.0980,  ..., 0.0941, 0.0941, 0.0275],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0118, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0784, 0.0941, 0.0941,  ..., 0.0902, 0.0980, 0.0275],\n",
       "          [0.0784, 0.0941, 0.0941,  ..., 0.0941, 0.0941, 0.0275],\n",
       "          [0.0745, 0.0941, 0.0980,  ..., 0.0941, 0.0941, 0.0275],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0118, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73506067",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00eee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae62564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomNeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LeNet = nn.Sequential(     \n",
    "            # convolutional layers            \n",
    "            nn.Sequential(                                            # FIRST LAYER: (INPUT LAYER)\n",
    "              nn.Conv2d(3, 6, kernel_size=3, stride=1, padding=0),    # CONVOLUTION \n",
    "              nn.BatchNorm2d(6),\n",
    "              nn.Softmax(dim=1),\n",
    "              nn.Dropout(p=dropout_rate),\n",
    "              nn.MaxPool2d(kernel_size=2, stride=2)),                # POOLING\n",
    "            nn.Sequential(                                            # SECOND LAYER: HIDDEN LAYER 1\n",
    "              nn.Conv2d(6, 16, kernel_size=3, stride=1, padding=0),   # CONVOLUTION \n",
    "              nn.BatchNorm2d(16),\n",
    "              nn.Softmax(dim=1),\n",
    "              nn.Dropout(p=dropout_rate),\n",
    "              nn.MaxPool2d(kernel_size=2, stride=2)),                # POOLING\n",
    "            # fully connected layers\n",
    "            nn.Flatten(),\n",
    "            # output layer\n",
    "            nn.Linear(16 * 54 * 54, 2)                                # OUTPUT LAYER\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.LeNet(x)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a1ebc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = CustomNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2759df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_1.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7213dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.1017, Train Acc: 0.9663 | Val Loss: 0.5162, Val Acc: 0.8125\n",
      "Epoch 2/20 | Train Loss: 0.0907, Train Acc: 0.9713 | Val Loss: 0.6804, Val Acc: 0.5000\n",
      "Epoch 3/20 | Train Loss: 0.0835, Train Acc: 0.9720 | Val Loss: 0.5998, Val Acc: 0.5000\n",
      "Epoch 4/20 | Train Loss: 0.0762, Train Acc: 0.9730 | Val Loss: 0.5667, Val Acc: 0.6250\n",
      "Epoch 5/20 | Train Loss: 0.0716, Train Acc: 0.9773 | Val Loss: 0.4982, Val Acc: 0.7500\n",
      "Epoch 6/20 | Train Loss: 0.0704, Train Acc: 0.9768 | Val Loss: 0.4728, Val Acc: 0.7500\n",
      "Epoch 7/20 | Train Loss: 0.0661, Train Acc: 0.9782 | Val Loss: 0.5531, Val Acc: 0.6250\n",
      "Epoch 8/20 | Train Loss: 0.0604, Train Acc: 0.9804 | Val Loss: 0.4017, Val Acc: 0.8750\n",
      "Epoch 9/20 | Train Loss: 0.0571, Train Acc: 0.9816 | Val Loss: 0.4853, Val Acc: 0.8125\n",
      "Epoch 10/20 | Train Loss: 0.0528, Train Acc: 0.9844 | Val Loss: 0.6948, Val Acc: 0.5000\n",
      "Epoch 11/20 | Train Loss: 0.0513, Train Acc: 0.9844 | Val Loss: 0.7799, Val Acc: 0.5000\n",
      "Epoch 12/20 | Train Loss: 0.0505, Train Acc: 0.9842 | Val Loss: 0.3926, Val Acc: 0.9375\n",
      "Epoch 13/20 | Train Loss: 0.0481, Train Acc: 0.9838 | Val Loss: 0.3511, Val Acc: 0.8750\n",
      "Epoch 14/20 | Train Loss: 0.0441, Train Acc: 0.9849 | Val Loss: 0.3279, Val Acc: 0.9375\n",
      "Epoch 15/20 | Train Loss: 0.0406, Train Acc: 0.9877 | Val Loss: 0.3167, Val Acc: 1.0000\n",
      "Epoch 16/20 | Train Loss: 0.0400, Train Acc: 0.9877 | Val Loss: 0.3746, Val Acc: 0.8125\n",
      "Epoch 17/20 | Train Loss: 0.0375, Train Acc: 0.9896 | Val Loss: 0.4695, Val Acc: 0.8125\n",
      "Epoch 18/20 | Train Loss: 0.0387, Train Acc: 0.9889 | Val Loss: 0.3311, Val Acc: 0.8750\n",
      "Epoch 19/20 | Train Loss: 0.0343, Train Acc: 0.9906 | Val Loss: 0.3432, Val Acc: 0.9375\n",
      "Epoch 20/20 | Train Loss: 0.0371, Train Acc: 0.9886 | Val Loss: 0.3215, Val Acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "    # TRAIN\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model_1.train()\n",
    "    running_loss = 0.0\n",
    "    running_matched = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data           # NOTE: depending on how you implemented your dataset class's __getitem__ it could be labels, inputs\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # CALCULATE ACCURACY METRIC\n",
    "        _, preds = torch.max(outputs, 1)  # Find out the predicted class with the highest prob\n",
    "        running_matched += torch.sum(preds == labels.data) # caculate the number of matched labels\n",
    "\n",
    "    avg_train_loss = running_loss / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
    "    avg_train_acc = running_matched.double() / len(train_dataloader.dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "\n",
    "    # VALIDATE\n",
    "    # In the validation part, we don't want to keep track of the gradients \n",
    "    model_1.eval()\n",
    "    running_val_loss = 0.0\n",
    "    running_val_matched = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs = model_1(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # keep track of the loss\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # CALCULATE ACCURACY METRIC\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_val_matched += torch.sum(preds == labels.data)\n",
    "\n",
    "    avg_val_loss = running_val_loss / (i + 1)\n",
    "    avg_val_acc = running_val_matched.double() / len(val_dataloader.dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(avg_val_acc)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ba52754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.9663, dtype=torch.float64),\n",
       " tensor(0.9713, dtype=torch.float64),\n",
       " tensor(0.9720, dtype=torch.float64),\n",
       " tensor(0.9730, dtype=torch.float64),\n",
       " tensor(0.9773, dtype=torch.float64),\n",
       " tensor(0.9768, dtype=torch.float64),\n",
       " tensor(0.9782, dtype=torch.float64),\n",
       " tensor(0.9804, dtype=torch.float64),\n",
       " tensor(0.9816, dtype=torch.float64),\n",
       " tensor(0.9844, dtype=torch.float64),\n",
       " tensor(0.9844, dtype=torch.float64),\n",
       " tensor(0.9842, dtype=torch.float64),\n",
       " tensor(0.9838, dtype=torch.float64),\n",
       " tensor(0.9849, dtype=torch.float64),\n",
       " tensor(0.9877, dtype=torch.float64),\n",
       " tensor(0.9877, dtype=torch.float64),\n",
       " tensor(0.9896, dtype=torch.float64),\n",
       " tensor(0.9889, dtype=torch.float64),\n",
       " tensor(0.9906, dtype=torch.float64),\n",
       " tensor(0.9886, dtype=torch.float64)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fba340c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10173026018685871,\n",
       " 0.09072341884095822,\n",
       " 0.08354679867625237,\n",
       " 0.07615594978955004,\n",
       " 0.07161806333028982,\n",
       " 0.07035121841416243,\n",
       " 0.06614582539506016,\n",
       " 0.06039045020307024,\n",
       " 0.05711717502544484,\n",
       " 0.052785396883865035,\n",
       " 0.051331473702204325,\n",
       " 0.05054859138063846,\n",
       " 0.04812943094199704,\n",
       " 0.044066273877697605,\n",
       " 0.04058976383549311,\n",
       " 0.040024005011805606,\n",
       " 0.0375443015457882,\n",
       " 0.038703201511394114,\n",
       " 0.034343408585916606,\n",
       " 0.037115964159790064]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a782d0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.8125, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.6250, dtype=torch.float64),\n",
       " tensor(0.7500, dtype=torch.float64),\n",
       " tensor(0.7500, dtype=torch.float64),\n",
       " tensor(0.6250, dtype=torch.float64),\n",
       " tensor(0.8750, dtype=torch.float64),\n",
       " tensor(0.8125, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.9375, dtype=torch.float64),\n",
       " tensor(0.8750, dtype=torch.float64),\n",
       " tensor(0.9375, dtype=torch.float64),\n",
       " tensor(1., dtype=torch.float64),\n",
       " tensor(0.8125, dtype=torch.float64),\n",
       " tensor(0.8125, dtype=torch.float64),\n",
       " tensor(0.8750, dtype=torch.float64),\n",
       " tensor(0.9375, dtype=torch.float64),\n",
       " tensor(0.9375, dtype=torch.float64)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae8aaa5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5162,\n",
       " 0.6804,\n",
       " 0.5998,\n",
       " 0.5667,\n",
       " 0.4982,\n",
       " 0.4728,\n",
       " 0.5531,\n",
       " 0.4017,\n",
       " 0.4853,\n",
       " 0.6948,\n",
       " 0.7799,\n",
       " 0.3926,\n",
       " 0.3511,\n",
       " 0.3279,\n",
       " 0.3167,\n",
       " 0.3746,\n",
       " 0.4695,\n",
       " 0.3311,\n",
       " 0.3432,\n",
       " 0.3215]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79d1a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_1.state_dict(), './softmaxmodel.PT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a9bf934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load do this:\n",
    "#model = torch.load('./softmaxmodel.PT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5115cdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.42320847511291504\n",
      "Test Acc: 0.875\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "\n",
    "model_1.eval()\n",
    "running_test_loss = 0.0\n",
    "running_test_matched = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        outputs = model_1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # keep track of the loss\n",
    "        running_test_loss += loss.item()\n",
    "\n",
    "        # CALCULATE ACCURACY METRIC\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_test_matched += torch.sum(preds == labels.data)\n",
    "\n",
    "avg_test_loss = running_test_loss\n",
    "avg_test_acc = running_test_matched.double() / len(test_dataloader.dataset)\n",
    "\n",
    "# Print epoch summary\n",
    "print(\"Test Loss: {}\".format(avg_test_loss))\n",
    "print(\"Test Acc: {}\".format(avg_test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
