{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd354e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ffe49",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab8631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './data/chest_xray/'\n",
    "train_path = './data/chest_xray/'\n",
    "val_path = './data/chest_xray/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c700420",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_info = pd.read_csv('img_info.csv')\n",
    "test_df = img_info.loc[img_info.loc[:, 'data'] == 'Test']\n",
    "train_df = img_info.loc[img_info.loc[:, 'data'] == 'Train']\n",
    "val_df = test_df = img_info.loc[img_info.loc[:, 'data'] == 'Validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3cb87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Normal Images in the Training Data: 0.5093694606229425\n",
      "Percentage of Pneumonia Images in the Training Data: 0.4906305393770575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/dwj3n8716t593gn_r4x21y0r0000gn/T/ipykernel_71949/2152691127.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  normal_images.loc[:,'imbalance'] = 1\n"
     ]
    }
   ],
   "source": [
    "# Apply data augmentaion to solve the imbance problem\n",
    "normal_images = train_df.loc[train_df.loc[:,'label'] == 0]\n",
    "normal_images.loc[:,'imbalance'] = 1\n",
    "train_df = pd.concat([train_df, normal_images, normal_images]).reset_index(drop=True)\n",
    "print('Percentage of Normal Images in the Training Data: {}'.format(\n",
    "    len(train_df.loc[train_df.loc[:,'label'] == 0])/len(train_df.loc[:,'label'])))\n",
    "print('Percentage of Pneumonia Images in the Training Data: {}'.format(\n",
    "    len(train_df.loc[train_df.loc[:,'label'] == 1])/len(train_df.loc[:,'label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4709732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, img_dir_path, transforms=None):\n",
    "        \"\"\"\n",
    "        You can set your custom dataset to take in more parameters than specified\n",
    "        here. But, I recommend at least you start with the three I listed here,\n",
    "        as these are standard\n",
    "\n",
    "        csv_file (str): file path to the csv file you created /\n",
    "        df (pandas df): pandas dataframe\n",
    "\n",
    "        img_dir_path: directory path to your images\n",
    "        transform: Compose (a PyTorch Class) that strings together several\n",
    "          transform functions (e.g. data augmentation steps)\n",
    "\n",
    "        One thing to note -- you technically could implement `transform` within\n",
    "        the dataset. No one is going to stop you, but you can think of the\n",
    "        transformations/augmentations you do as a hyperparameter. If you treat\n",
    "        it as a hyperparameter, you want to be able to experiment with different\n",
    "        transformations, and therefore, it would make more sense to decide those\n",
    "        transformations outside the dataset class and pass it to the dataset!\n",
    "        \"\"\"\n",
    "        self.img_labels = df\n",
    "        self.img_dir = img_dir_path\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns: (int) length of your dataset\n",
    "        \"\"\"\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and returns your sample (the image and the label) at the\n",
    "        specified index\n",
    "\n",
    "        Parameter: idx (int): index of interest\n",
    "\n",
    "        Returns: image, label\n",
    "        \"\"\"\n",
    "\n",
    "        img_path =  self.img_dir + self.img_labels.iloc[idx, 1]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        label = self.img_labels.iloc[idx, -1]\n",
    "        \n",
    "        imbalance = self.img_labels.iloc[idx, -2]\n",
    "\n",
    "        if self.transforms:\n",
    "            \n",
    "            if imbalance and not label:\n",
    "                image = transforms(image)\n",
    "                image = imbalance_transform(image)\n",
    "                \n",
    "            else:\n",
    "                image = transforms(image)\n",
    "        else:\n",
    "            image = test_transform(image)\n",
    "               \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e902f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose(\n",
    "    [\n",
    "        T.Resize((224,224), antialias=None, interpolation=InterpolationMode.BICUBIC),\n",
    "        T.RandomApply([\n",
    "            T.GaussianBlur(kernel_size=(5,5), sigma=(0.1, 0.2))\n",
    "        ], p=0.5),\n",
    "        T.RandomEqualize(),\n",
    "        T.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "tranform_test = T.Compose(\n",
    "    [\n",
    "        T.Resize((224,224), antialias=None, interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor()\n",
    "    ]\n",
    ")\n",
    "imbalance_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(degrees=10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    T.RandomErasing(p=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d00156bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomImageDataset(train_df, train_path, transforms=transforms)\n",
    "val_data = CustomImageDataset(val_df, val_path, transforms=transforms)\n",
    "test_data = CustomImageDataset(test_df, test_path, transforms=None)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e36bfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0941, 0.1098, 0.1098,  ..., 0.1059, 0.1137, 0.0392],\n",
       "          [0.0941, 0.1098, 0.1098,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          [0.0902, 0.1098, 0.1137,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0941, 0.1098, 0.1098,  ..., 0.1059, 0.1137, 0.0392],\n",
       "          [0.0941, 0.1098, 0.1098,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          [0.0902, 0.1098, 0.1137,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0941, 0.1098, 0.1098,  ..., 0.1059, 0.1137, 0.0392],\n",
       "          [0.0941, 0.1098, 0.1098,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          [0.0902, 0.1098, 0.1137,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73506067",
   "metadata": {},
   "source": [
    "## Model Setup - more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00eee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20fea7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_dataloader, val_dataloader, scheduler=None, EPOCHS=25):\n",
    "    \"\"\"\n",
    "    Train the model and return relevant accuracy metric\n",
    "    \n",
    "    Input:s\n",
    "        model: the defined neural network\n",
    "        criterion: the defined loss func\n",
    "        optimizer: the defined optimizer\n",
    "        scheduler: if specified, the learning rate is going to decrease with each epoch\n",
    "        train_dataloader & val_dataloader: DataLoader object\n",
    "        EPOCHS: int\n",
    "    Output:\n",
    "        model: trained model\n",
    "        train_losses, train_accuracies, val_losses, val_accuracies: acc metrics\n",
    "    \"\"\"\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    learning_rate = []\n",
    "\n",
    "    for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "        # TRAIN\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_matched = 0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data           # NOTE: depending on how you implemented your dataset class's __getitem__ it could be labels, inputs\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # keep track of the loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # CALCULATE ACCURACY METRIC\n",
    "            preds = torch.argmax(outputs, dim =1)  # Find out the index(label) of output with higher probablity\n",
    "            running_matched += torch.sum(preds == labels.data) # caculate the number of matched labels\n",
    "\n",
    "        avg_train_loss = running_loss / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
    "        avg_train_acc = running_matched.double() / len(train_dataloader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(avg_train_acc)\n",
    "\n",
    "        # VALIDATE\n",
    "        # In the validation part, we don't want to keep track of the gradients \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_matched = 0\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # keep track of the loss\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # CALCULATE ACCURACY METRIC\n",
    "                preds = torch.argmax(outputs, dim = 1)\n",
    "                running_val_matched += torch.sum(preds == labels.data)\n",
    "                true_labels.extend(labels.numpy())\n",
    "                predicted_labels.extend(preds.numpy())\n",
    "\n",
    "        avg_val_loss = running_val_loss / (i + 1)\n",
    "        avg_val_acc = running_val_matched.double() / len(val_dataloader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(avg_val_acc)\n",
    "        learning_rate.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        if scheduler != None:\n",
    "            scheduler.step(avg_val_loss) # add a scheduler to reduce the lr\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f} | \"\n",
    "              f\"learning rate: {optimizer.param_groups[0]['lr']:.4f}\")\n",
    "        \n",
    "        precision = precision_score(true_labels, predicted_labels)\n",
    "        recall = recall_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 score: {f1:.2f}\")\n",
    "        \n",
    "        \n",
    "    return model, train_losses, train_accuracies, val_losses, val_accuracies, learning_rate\n",
    "\n",
    "\n",
    "def test(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    Test the data and return test accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    running_test_matched = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim = 1)\n",
    "            true_labels.extend(labels.numpy())\n",
    "            predicted_labels.extend(preds.numpy())\n",
    "            running_test_matched += torch.sum(preds == labels.data)\n",
    "    \n",
    "        test_acc = running_test_matched.double() / len(test_dataloader.dataset)\n",
    "        precision = precision_score(true_labels, predicted_labels)\n",
    "        recall = recall_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 score: {f1:.2f}\")\n",
    "\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33af4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_model_and_results(model, \n",
    "                            train_losses, \n",
    "                            train_accuracies, \n",
    "                            val_losses, \n",
    "                            val_accuracies,\n",
    "                            learning_rate,\n",
    "                            path):\n",
    "    \"\"\"\n",
    "    Save the model and results\n",
    "    \n",
    "    Inputs:\n",
    "        path: a list of str containing the path for model and the path for results\n",
    "    \"\"\"\n",
    "\n",
    "    train_acc = [float(i) for i in train_accuracies]\n",
    "    val_acc = [float(i) for i in val_accuracies]\n",
    "\n",
    "    result = {'train_losses': train_losses,\n",
    "             'train_accuracies': train_acc,\n",
    "             'val_losses': val_losses,\n",
    "             'val_accuracies': val_acc,\n",
    "             'learning_rates': learning_rate}\n",
    "    \n",
    "    # save model\n",
    "    torch.save(model.state_dict(), path[0])\n",
    "    # save results\n",
    "    with open(path[1], 'w') as f:\n",
    "        f.write(json.dumps(result))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163e19a",
   "metadata": {},
   "source": [
    "### Sample Model Construction\n",
    "\n",
    "Here shows the training precess for the CNN with ReLU as the activation function and Negative Loss Likelihood as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9197b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a three-layer network\n",
    "class CustomNeuralNetwork_relu(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.LeNet = nn.Sequential(     \n",
    "            # convolutional layers            \n",
    "           \n",
    "            nn.Sequential(                                            # FIRST LAYER: (INPUT LAYER)\n",
    "              nn.Conv2d(3, 6, kernel_size=3, stride=1, padding=0),    # CONVOLUTION \n",
    "              nn.BatchNorm2d(6),\n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(p=dropout_rate),\n",
    "              nn.MaxPool2d(kernel_size = 2, stride = 2)),             # POOLING\n",
    "           \n",
    "            nn.Sequential(                                            # SECOND LAYER: HIDDEN LAYER 1\n",
    "              nn.Conv2d(6, 16, kernel_size=3, stride=1, padding=0),   # CONVOLUTION \n",
    "              nn.BatchNorm2d(16),\n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(p=dropout_rate),\n",
    "              nn.MaxPool2d(kernel_size = 2, stride = 2)),             # POOLING\n",
    "            # fully connected layers\n",
    "            nn.Flatten(),\n",
    "            # output layer\n",
    "            nn.Linear(16 * 54 * 54, 2)                                # OUTPUT LAYER\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.LeNet(x)\n",
    "        \n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c8e0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu = CustomNeuralNetwork_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cceffe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model_relu.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Inspired by: https://hasty.ai/docs/mp-wiki/scheduler/reducelronplateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                 mode='min', \n",
    "                                                 factor=0.5, \n",
    "                                                 patience=5, \n",
    "                                                 threshold=0.0001, \n",
    "                                                 threshold_mode='rel', \n",
    "                                                 cooldown=0, \n",
    "                                                 min_lr=0, \n",
    "                                                 eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7f9c3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.0256, Train Acc: 0.9915 | Val Loss: 0.6184, Val Acc: 0.6875 | learning rate: 0.0010\n",
      "Precision: 0.62, Recall: 1.00, F1 score: 0.76\n",
      "Epoch 2/15 | Train Loss: 0.0303, Train Acc: 0.9901 | Val Loss: 0.7073, Val Acc: 0.6875 | learning rate: 0.0010\n",
      "Precision: 0.62, Recall: 1.00, F1 score: 0.76\n",
      "Epoch 3/15 | Train Loss: 0.0255, Train Acc: 0.9919 | Val Loss: 0.4428, Val Acc: 0.8125 | learning rate: 0.0010\n",
      "Precision: 0.73, Recall: 1.00, F1 score: 0.84\n",
      "Epoch 4/15 | Train Loss: 0.0354, Train Acc: 0.9887 | Val Loss: 0.2712, Val Acc: 0.8125 | learning rate: 0.0010\n",
      "Precision: 0.73, Recall: 1.00, F1 score: 0.84\n",
      "Epoch 5/15 | Train Loss: 0.0204, Train Acc: 0.9932 | Val Loss: 0.3247, Val Acc: 0.8125 | learning rate: 0.0010\n",
      "Precision: 0.73, Recall: 1.00, F1 score: 0.84\n",
      "Epoch 6/15 | Train Loss: 0.0240, Train Acc: 0.9927 | Val Loss: 0.2620, Val Acc: 0.8750 | learning rate: 0.0010\n",
      "Precision: 0.80, Recall: 1.00, F1 score: 0.89\n",
      "Epoch 7/15 | Train Loss: 0.0183, Train Acc: 0.9934 | Val Loss: 0.2776, Val Acc: 0.8750 | learning rate: 0.0010\n",
      "Precision: 0.80, Recall: 1.00, F1 score: 0.89\n",
      "Epoch 8/15 | Train Loss: 0.0176, Train Acc: 0.9939 | Val Loss: 0.2591, Val Acc: 0.8750 | learning rate: 0.0010\n",
      "Precision: 0.80, Recall: 1.00, F1 score: 0.89\n",
      "Epoch 9/15 | Train Loss: 0.0116, Train Acc: 0.9961 | Val Loss: 0.2650, Val Acc: 0.8750 | learning rate: 0.0010\n",
      "Precision: 0.80, Recall: 1.00, F1 score: 0.89\n",
      "Epoch 10/15 | Train Loss: 0.0166, Train Acc: 0.9943 | Val Loss: 0.1499, Val Acc: 1.0000 | learning rate: 0.0010\n",
      "Precision: 1.00, Recall: 1.00, F1 score: 1.00\n",
      "Epoch 11/15 | Train Loss: 0.0139, Train Acc: 0.9949 | Val Loss: 0.1262, Val Acc: 0.9375 | learning rate: 0.0010\n",
      "Precision: 0.89, Recall: 1.00, F1 score: 0.94\n",
      "Epoch 12/15 | Train Loss: 0.0170, Train Acc: 0.9947 | Val Loss: 0.4722, Val Acc: 0.7500 | learning rate: 0.0010\n",
      "Precision: 0.67, Recall: 1.00, F1 score: 0.80\n",
      "Epoch 13/15 | Train Loss: 0.0168, Train Acc: 0.9929 | Val Loss: 0.1392, Val Acc: 0.9375 | learning rate: 0.0010\n",
      "Precision: 0.89, Recall: 1.00, F1 score: 0.94\n",
      "Epoch 14/15 | Train Loss: 0.0299, Train Acc: 0.9899 | Val Loss: 0.2659, Val Acc: 0.8750 | learning rate: 0.0010\n",
      "Precision: 0.80, Recall: 1.00, F1 score: 0.89\n",
      "Epoch 15/15 | Train Loss: 0.0158, Train Acc: 0.9937 | Val Loss: 0.1807, Val Acc: 0.8750 | learning rate: 0.0010\n",
      "Precision: 0.80, Recall: 1.00, F1 score: 0.89\n"
     ]
    }
   ],
   "source": [
    "# I interupt running after this first epoch, feel free to continue run it\n",
    "model_relu, train_losses, train_accuracies, val_losses, val_accuracies, learning_rate = train(model_relu, \n",
    "                                                                                              criterion, \n",
    "                                                                                              optimizer,\n",
    "                                                                                              train_dataloader, \n",
    "                                                                                              val_dataloader, \n",
    "                                                                                              scheduler,\n",
    "                                                                                              EPOCHS=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "282d5d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.89, Recall: 1.00, F1 score: 0.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9375, dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_relu, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42979fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ['model_relu_NLL_15+patience', 'result_relu_NLL_15+patience.txt']\n",
    "write_model_and_results(model_relu, train_losses, train_accuracies, val_losses, val_accuracies, learning_rate, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3784275",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08975a",
   "metadata": {},
   "source": [
    "**Note:** We incorporate all the accuracy metrics generated by the trained models into two files: CNN_crossentropy_output.txt and CNN_NLL_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912c805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
