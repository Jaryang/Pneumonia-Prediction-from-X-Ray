{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd354e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ffe49",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab8631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/chest_xray/test'\n",
    "train_path = 'data/chest_xray/train'\n",
    "val_path = 'data/chest_xray/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b597412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_info = pd.read_csv('./img_info.csv')\n",
    "test_df = img_info.loc[img_info.loc[:, 'data'] == 'Test']\n",
    "train_df = img_info.loc[img_info.loc[:, 'data'] == 'Train']\n",
    "val_df = test_df = img_info.loc[img_info.loc[:, 'data'] == 'Validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d71c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Normal Images in the Training Data: 0.5093694606229425\n",
      "Percentage of Pneumonia Images in the Training Data: 0.4906305393770575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/qwgd8hzn23n90l6twpb5dty40000gn/T/ipykernel_47421/907957275.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  normal_images.loc[:,'imbalance'] = 1\n"
     ]
    }
   ],
   "source": [
    "normal_images = train_df.loc[train_df.loc[:,'label'] == 0]\n",
    "normal_images.loc[:,'imbalance'] = 1\n",
    "train_df = pd.concat([train_df, normal_images, normal_images]).reset_index(drop=True)\n",
    "print('Percentage of Normal Images in the Training Data: {}'.format(\n",
    "    len(train_df.loc[train_df.loc[:,'label'] == 0])/len(train_df.loc[:,'label'])))\n",
    "print('Percentage of Pneumonia Images in the Training Data: {}'.format(\n",
    "    len(train_df.loc[train_df.loc[:,'label'] == 1])/len(train_df.loc[:,'label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4709732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, img_dir_path, transforms=None):\n",
    "        \"\"\"\n",
    "        You can set your custom dataset to take in more parameters than specified\n",
    "        here. But, I recommend at least you start with the three I listed here,\n",
    "        as these are standard\n",
    "\n",
    "        csv_file (str): file path to the csv file you created /\n",
    "        df (pandas df): pandas dataframe\n",
    "\n",
    "        img_dir_path: directory path to your images\n",
    "        transform: Compose (a PyTorch Class) that strings together several\n",
    "          transform functions (e.g. data augmentation steps)\n",
    "\n",
    "        One thing to note -- you technically could implement `transform` within\n",
    "        the dataset. No one is going to stop you, but you can think of the\n",
    "        transformations/augmentations you do as a hyperparameter. If you treat\n",
    "        it as a hyperparameter, you want to be able to experiment with different\n",
    "        transformations, and therefore, it would make more sense to decide those\n",
    "        transformations outside the dataset class and pass it to the dataset!\n",
    "        \"\"\"\n",
    "        self.img_labels = df\n",
    "        self.img_dir = img_dir_path\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns: (int) length of your dataset\n",
    "        \"\"\"\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and returns your sample (the image and the label) at the\n",
    "        specified index\n",
    "\n",
    "        Parameter: idx (int): index of interest\n",
    "\n",
    "        Returns: image, label\n",
    "        \"\"\"\n",
    "\n",
    "        img_path =  self.img_labels.iloc[idx, 1]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        label = self.img_labels.iloc[idx, -1]\n",
    "        \n",
    "        imbalance = self.img_labels.iloc[idx, -2]\n",
    "\n",
    "        if self.transforms:\n",
    "            \n",
    "            if imbalance and not label:\n",
    "                image = transforms(image)\n",
    "                image = imbalance_transform(image)\n",
    "                \n",
    "            else:\n",
    "                image = transforms(image)\n",
    "               \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834e7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose(\n",
    "    [\n",
    "        T.Resize((224,224), antialias=None, interpolation=InterpolationMode.BICUBIC),\n",
    "        T.RandomApply([\n",
    "            T.GaussianBlur(kernel_size=(5,5), sigma=(0.1, 0.2))\n",
    "        ], p=0.5),\n",
    "        T.RandomEqualize(),\n",
    "        T.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "imbalance_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(degrees=10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    T.RandomErasing(p=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00156bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomImageDataset(train_df, train_path, transforms=transforms)\n",
    "val_data = CustomImageDataset(val_df, val_path, transforms=transforms)\n",
    "test_data = CustomImageDataset(test_df, test_path, transforms=transforms)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae732e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0941, 0.1098, 0.1098,  ..., 0.1059, 0.1137, 0.0392],\n",
       "          [0.0941, 0.1098, 0.1098,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          [0.0902, 0.1098, 0.1137,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0941, 0.1098, 0.1098,  ..., 0.1059, 0.1137, 0.0392],\n",
       "          [0.0941, 0.1098, 0.1098,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          [0.0902, 0.1098, 0.1137,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0941, 0.1098, 0.1098,  ..., 0.1059, 0.1137, 0.0392],\n",
       "          [0.0941, 0.1098, 0.1098,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          [0.0902, 0.1098, 0.1137,  ..., 0.1098, 0.1098, 0.0392],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73506067",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00eee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae62564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomNeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LeNet = nn.Sequential(     \n",
    "            # convolutional layers            \n",
    "            nn.Sequential(                                            # FIRST LAYER: (INPUT LAYER)\n",
    "              nn.Conv2d(3, 6, kernel_size=3, stride=1, padding=0),    # CONVOLUTION \n",
    "              nn.BatchNorm2d(6),\n",
    "              nn.Softmax(dim=1),\n",
    "              nn.Dropout(p=dropout_rate),\n",
    "              nn.MaxPool2d(kernel_size=2, stride=2)),                # POOLING\n",
    "            nn.Sequential(                                            # SECOND LAYER: HIDDEN LAYER 1\n",
    "              nn.Conv2d(6, 16, kernel_size=3, stride=1, padding=0),   # CONVOLUTION \n",
    "              nn.BatchNorm2d(16),\n",
    "              nn.Softmax(dim=1),\n",
    "              nn.Dropout(p=dropout_rate),\n",
    "              nn.MaxPool2d(kernel_size=2, stride=2)),                # POOLING\n",
    "            # fully connected layers\n",
    "            nn.Flatten(),\n",
    "            # output layer\n",
    "            nn.Linear(16 * 54 * 54, 2)                                # OUTPUT LAYER\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.LeNet(x)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35269800",
   "metadata": {},
   "source": [
    "## Softmax + Negative Log Likelihood Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b8f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomNeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LeNet = nn.Sequential(     \n",
    "            # convolutional layers            \n",
    "            nn.Sequential(                                            # FIRST LAYER: (INPUT LAYER)\n",
    "              nn.Conv2d(3, 6, kernel_size=3, stride=1, padding=0),    # CONVOLUTION \n",
    "              nn.BatchNorm2d(6),\n",
    "              nn.Softmax(dim=1),\n",
    "              nn.Dropout(p=dropout_rate),\n",
    "              nn.MaxPool2d(kernel_size=2, stride=2)),                # POOLING\n",
    "            nn.Sequential(                                            # SECOND LAYER: HIDDEN LAYER 1\n",
    "              nn.Conv2d(6, 16, kernel_size=3, stride=1, padding=0),   # CONVOLUTION \n",
    "              nn.BatchNorm2d(16),\n",
    "              nn.Softmax(dim=1),\n",
    "              nn.Dropout(p=dropout_rate),\n",
    "              nn.MaxPool2d(kernel_size=2, stride=2)),                # POOLING\n",
    "            # fully connected layers\n",
    "            nn.Flatten(),\n",
    "            # output layer\n",
    "            nn.Linear(16 * 54 * 54, 2)                                # OUTPUT LAYER\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.LeNet(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69aeea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nll = CustomNeuralNetwork()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model_nll.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c0c2ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.3262, Train Acc: 0.8822 | Val Loss: 0.6457, Val Acc: 0.5625\n",
      "Epoch 2/20 | Train Loss: 0.1758, Train Acc: 0.9410 | Val Loss: 0.8021, Val Acc: 0.5000\n",
      "Epoch 3/20 | Train Loss: 0.1386, Train Acc: 0.9545 | Val Loss: 0.7471, Val Acc: 0.5000\n",
      "Epoch 4/20 | Train Loss: 0.1186, Train Acc: 0.9602 | Val Loss: 0.8547, Val Acc: 0.5000\n",
      "Epoch 5/20 | Train Loss: 0.1085, Train Acc: 0.9657 | Val Loss: 0.8934, Val Acc: 0.5000\n",
      "Epoch 6/20 | Train Loss: 0.0995, Train Acc: 0.9675 | Val Loss: 0.8840, Val Acc: 0.5000\n",
      "Epoch 7/20 | Train Loss: 0.0924, Train Acc: 0.9700 | Val Loss: 0.9567, Val Acc: 0.5000\n",
      "Epoch 8/20 | Train Loss: 0.0861, Train Acc: 0.9727 | Val Loss: 0.9420, Val Acc: 0.5000\n",
      "Epoch 9/20 | Train Loss: 0.0798, Train Acc: 0.9728 | Val Loss: 0.8896, Val Acc: 0.5000\n",
      "Epoch 10/20 | Train Loss: 0.0748, Train Acc: 0.9751 | Val Loss: 1.0771, Val Acc: 0.5000\n",
      "Epoch 11/20 | Train Loss: 0.0736, Train Acc: 0.9762 | Val Loss: 0.9686, Val Acc: 0.5000\n",
      "Epoch 12/20 | Train Loss: 0.0715, Train Acc: 0.9751 | Val Loss: 1.1260, Val Acc: 0.5000\n",
      "Epoch 13/20 | Train Loss: 0.0654, Train Acc: 0.9786 | Val Loss: 1.1333, Val Acc: 0.5000\n",
      "Epoch 14/20 | Train Loss: 0.0608, Train Acc: 0.9823 | Val Loss: 0.9450, Val Acc: 0.5000\n",
      "Epoch 15/20 | Train Loss: 0.0610, Train Acc: 0.9796 | Val Loss: 1.3621, Val Acc: 0.5000\n",
      "Epoch 16/20 | Train Loss: 0.0573, Train Acc: 0.9805 | Val Loss: 0.9776, Val Acc: 0.5000\n",
      "Epoch 17/20 | Train Loss: 0.0540, Train Acc: 0.9827 | Val Loss: 1.1788, Val Acc: 0.5000\n",
      "Epoch 18/20 | Train Loss: 0.0507, Train Acc: 0.9847 | Val Loss: 1.2030, Val Acc: 0.5000\n",
      "Epoch 19/20 | Train Loss: 0.0525, Train Acc: 0.9847 | Val Loss: 1.1843, Val Acc: 0.5000\n",
      "Epoch 20/20 | Train Loss: 0.0507, Train Acc: 0.9839 | Val Loss: 1.1704, Val Acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "    # TRAIN\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model_nll.train()\n",
    "    running_loss = 0.0\n",
    "    running_matched = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data           # NOTE: depending on how you implemented your dataset class's __getitem__ it could be labels, inputs\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_nll(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # CALCULATE ACCURACY METRIC\n",
    "        _, preds = torch.max(outputs, 1)  # Find out the predicted class with the highest prob\n",
    "        running_matched += torch.sum(preds == labels.data) # caculate the number of matched labels\n",
    "\n",
    "    avg_train_loss = running_loss / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
    "    avg_train_acc = running_matched.double() / len(train_dataloader.dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "\n",
    "    # VALIDATE\n",
    "    # In the validation part, we don't want to keep track of the gradients \n",
    "    model_nll.eval()\n",
    "    running_val_loss = 0.0\n",
    "    running_val_matched = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs = model_nll(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # keep track of the loss\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # CALCULATE ACCURACY METRIC\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_val_matched += torch.sum(preds == labels.data)\n",
    "\n",
    "    avg_val_loss = running_val_loss / (i + 1)\n",
    "    avg_val_acc = running_val_matched.double() / len(val_dataloader.dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(avg_val_acc)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f412da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_nll.state_dict(), './softmaxmodel_nll.PT')\n",
    "#To load do this:\n",
    "#model = torch.load('./softmaxmodel_nll.PT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a76e60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.8822, dtype=torch.float64),\n",
       " tensor(0.9410, dtype=torch.float64),\n",
       " tensor(0.9545, dtype=torch.float64),\n",
       " tensor(0.9602, dtype=torch.float64),\n",
       " tensor(0.9657, dtype=torch.float64),\n",
       " tensor(0.9675, dtype=torch.float64),\n",
       " tensor(0.9700, dtype=torch.float64),\n",
       " tensor(0.9727, dtype=torch.float64),\n",
       " tensor(0.9728, dtype=torch.float64),\n",
       " tensor(0.9751, dtype=torch.float64),\n",
       " tensor(0.9762, dtype=torch.float64),\n",
       " tensor(0.9751, dtype=torch.float64),\n",
       " tensor(0.9786, dtype=torch.float64),\n",
       " tensor(0.9823, dtype=torch.float64),\n",
       " tensor(0.9796, dtype=torch.float64),\n",
       " tensor(0.9805, dtype=torch.float64),\n",
       " tensor(0.9827, dtype=torch.float64),\n",
       " tensor(0.9847, dtype=torch.float64),\n",
       " tensor(0.9847, dtype=torch.float64),\n",
       " tensor(0.9839, dtype=torch.float64)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfae18e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32616804684362105,\n",
       " 0.1758161514036117,\n",
       " 0.138580015229602,\n",
       " 0.11861246169334458,\n",
       " 0.10846087183322638,\n",
       " 0.0995263522490859,\n",
       " 0.09239630869800045,\n",
       " 0.08612944155691131,\n",
       " 0.07978518612142052,\n",
       " 0.07481737751814146,\n",
       " 0.07360889199340055,\n",
       " 0.07151141511877218,\n",
       " 0.06544885727306528,\n",
       " 0.06079434483282028,\n",
       " 0.06095229207928623,\n",
       " 0.05734307994885791,\n",
       " 0.05401256589609529,\n",
       " 0.05071916461231247,\n",
       " 0.05251383390127411,\n",
       " 0.050730077947880474]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92bfe85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.5625, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67076e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6457382440567017,\n",
       " 0.8020821809768677,\n",
       " 0.7471135258674622,\n",
       " 0.8546653985977173,\n",
       " 0.8934195041656494,\n",
       " 0.8840035200119019,\n",
       " 0.9567467570304871,\n",
       " 0.9419949054718018,\n",
       " 0.8895538449287415,\n",
       " 1.0771489143371582,\n",
       " 0.9686247706413269,\n",
       " 1.1260128021240234,\n",
       " 1.133291482925415,\n",
       " 0.9450309872627258,\n",
       " 1.3620797395706177,\n",
       " 0.9776105880737305,\n",
       " 1.1788190603256226,\n",
       " 1.202968955039978,\n",
       " 1.184328317642212,\n",
       " 1.170414686203003]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ced147e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0381602048873901\n",
      "Test Acc: 0.5\n"
     ]
    }
   ],
   "source": [
    "model_nll.eval()\n",
    "running_test_loss = 0.0\n",
    "running_test_matched = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        outputs = model_nll(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # keep track of the loss\n",
    "        running_test_loss += loss.item()\n",
    "\n",
    "        # CALCULATE ACCURACY METRIC\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_test_matched += torch.sum(preds == labels.data)\n",
    "\n",
    "avg_test_loss = running_test_loss\n",
    "avg_test_acc = running_test_matched.double() / len(test_dataloader.dataset)\n",
    "\n",
    "# Print epoch summary\n",
    "print(\"Test Loss: {}\".format(avg_test_loss))\n",
    "print(\"Test Acc: {}\".format(avg_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f0010",
   "metadata": {},
   "source": [
    "## Softmax + Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a1ebc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = CustomNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2759df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_1.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7213dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.1017, Train Acc: 0.9663 | Val Loss: 0.5162, Val Acc: 0.8125\n",
      "Epoch 2/20 | Train Loss: 0.0907, Train Acc: 0.9713 | Val Loss: 0.6804, Val Acc: 0.5000\n",
      "Epoch 3/20 | Train Loss: 0.0835, Train Acc: 0.9720 | Val Loss: 0.5998, Val Acc: 0.5000\n",
      "Epoch 4/20 | Train Loss: 0.0762, Train Acc: 0.9730 | Val Loss: 0.5667, Val Acc: 0.6250\n",
      "Epoch 5/20 | Train Loss: 0.0716, Train Acc: 0.9773 | Val Loss: 0.4982, Val Acc: 0.7500\n",
      "Epoch 6/20 | Train Loss: 0.0704, Train Acc: 0.9768 | Val Loss: 0.4728, Val Acc: 0.7500\n",
      "Epoch 7/20 | Train Loss: 0.0661, Train Acc: 0.9782 | Val Loss: 0.5531, Val Acc: 0.6250\n",
      "Epoch 8/20 | Train Loss: 0.0604, Train Acc: 0.9804 | Val Loss: 0.4017, Val Acc: 0.8750\n",
      "Epoch 9/20 | Train Loss: 0.0571, Train Acc: 0.9816 | Val Loss: 0.4853, Val Acc: 0.8125\n",
      "Epoch 10/20 | Train Loss: 0.0528, Train Acc: 0.9844 | Val Loss: 0.6948, Val Acc: 0.5000\n",
      "Epoch 11/20 | Train Loss: 0.0513, Train Acc: 0.9844 | Val Loss: 0.7799, Val Acc: 0.5000\n",
      "Epoch 12/20 | Train Loss: 0.0505, Train Acc: 0.9842 | Val Loss: 0.3926, Val Acc: 0.9375\n",
      "Epoch 13/20 | Train Loss: 0.0481, Train Acc: 0.9838 | Val Loss: 0.3511, Val Acc: 0.8750\n",
      "Epoch 14/20 | Train Loss: 0.0441, Train Acc: 0.9849 | Val Loss: 0.3279, Val Acc: 0.9375\n",
      "Epoch 15/20 | Train Loss: 0.0406, Train Acc: 0.9877 | Val Loss: 0.3167, Val Acc: 1.0000\n",
      "Epoch 16/20 | Train Loss: 0.0400, Train Acc: 0.9877 | Val Loss: 0.3746, Val Acc: 0.8125\n",
      "Epoch 17/20 | Train Loss: 0.0375, Train Acc: 0.9896 | Val Loss: 0.4695, Val Acc: 0.8125\n",
      "Epoch 18/20 | Train Loss: 0.0387, Train Acc: 0.9889 | Val Loss: 0.3311, Val Acc: 0.8750\n",
      "Epoch 19/20 | Train Loss: 0.0343, Train Acc: 0.9906 | Val Loss: 0.3432, Val Acc: 0.9375\n",
      "Epoch 20/20 | Train Loss: 0.0371, Train Acc: 0.9886 | Val Loss: 0.3215, Val Acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "    # TRAIN\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model_1.train()\n",
    "    running_loss = 0.0\n",
    "    running_matched = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data           # NOTE: depending on how you implemented your dataset class's __getitem__ it could be labels, inputs\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # CALCULATE ACCURACY METRIC\n",
    "        _, preds = torch.max(outputs, 1)  # Find out the predicted class with the highest prob\n",
    "        running_matched += torch.sum(preds == labels.data) # caculate the number of matched labels\n",
    "\n",
    "    avg_train_loss = running_loss / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
    "    avg_train_acc = running_matched.double() / len(train_dataloader.dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "\n",
    "    # VALIDATE\n",
    "    # In the validation part, we don't want to keep track of the gradients \n",
    "    model_1.eval()\n",
    "    running_val_loss = 0.0\n",
    "    running_val_matched = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs = model_1(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # keep track of the loss\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # CALCULATE ACCURACY METRIC\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_val_matched += torch.sum(preds == labels.data)\n",
    "\n",
    "    avg_val_loss = running_val_loss / (i + 1)\n",
    "    avg_val_acc = running_val_matched.double() / len(val_dataloader.dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(avg_val_acc)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ba52754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.9663, dtype=torch.float64),\n",
       " tensor(0.9713, dtype=torch.float64),\n",
       " tensor(0.9720, dtype=torch.float64),\n",
       " tensor(0.9730, dtype=torch.float64),\n",
       " tensor(0.9773, dtype=torch.float64),\n",
       " tensor(0.9768, dtype=torch.float64),\n",
       " tensor(0.9782, dtype=torch.float64),\n",
       " tensor(0.9804, dtype=torch.float64),\n",
       " tensor(0.9816, dtype=torch.float64),\n",
       " tensor(0.9844, dtype=torch.float64),\n",
       " tensor(0.9844, dtype=torch.float64),\n",
       " tensor(0.9842, dtype=torch.float64),\n",
       " tensor(0.9838, dtype=torch.float64),\n",
       " tensor(0.9849, dtype=torch.float64),\n",
       " tensor(0.9877, dtype=torch.float64),\n",
       " tensor(0.9877, dtype=torch.float64),\n",
       " tensor(0.9896, dtype=torch.float64),\n",
       " tensor(0.9889, dtype=torch.float64),\n",
       " tensor(0.9906, dtype=torch.float64),\n",
       " tensor(0.9886, dtype=torch.float64)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fba340c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10173026018685871,\n",
       " 0.09072341884095822,\n",
       " 0.08354679867625237,\n",
       " 0.07615594978955004,\n",
       " 0.07161806333028982,\n",
       " 0.07035121841416243,\n",
       " 0.06614582539506016,\n",
       " 0.06039045020307024,\n",
       " 0.05711717502544484,\n",
       " 0.052785396883865035,\n",
       " 0.051331473702204325,\n",
       " 0.05054859138063846,\n",
       " 0.04812943094199704,\n",
       " 0.044066273877697605,\n",
       " 0.04058976383549311,\n",
       " 0.040024005011805606,\n",
       " 0.0375443015457882,\n",
       " 0.038703201511394114,\n",
       " 0.034343408585916606,\n",
       " 0.037115964159790064]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a782d0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.8125, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.6250, dtype=torch.float64),\n",
       " tensor(0.7500, dtype=torch.float64),\n",
       " tensor(0.7500, dtype=torch.float64),\n",
       " tensor(0.6250, dtype=torch.float64),\n",
       " tensor(0.8750, dtype=torch.float64),\n",
       " tensor(0.8125, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.5000, dtype=torch.float64),\n",
       " tensor(0.9375, dtype=torch.float64),\n",
       " tensor(0.8750, dtype=torch.float64),\n",
       " tensor(0.9375, dtype=torch.float64),\n",
       " tensor(1., dtype=torch.float64),\n",
       " tensor(0.8125, dtype=torch.float64),\n",
       " tensor(0.8125, dtype=torch.float64),\n",
       " tensor(0.8750, dtype=torch.float64),\n",
       " tensor(0.9375, dtype=torch.float64),\n",
       " tensor(0.9375, dtype=torch.float64)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae8aaa5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5162,\n",
       " 0.6804,\n",
       " 0.5998,\n",
       " 0.5667,\n",
       " 0.4982,\n",
       " 0.4728,\n",
       " 0.5531,\n",
       " 0.4017,\n",
       " 0.4853,\n",
       " 0.6948,\n",
       " 0.7799,\n",
       " 0.3926,\n",
       " 0.3511,\n",
       " 0.3279,\n",
       " 0.3167,\n",
       " 0.3746,\n",
       " 0.4695,\n",
       " 0.3311,\n",
       " 0.3432,\n",
       " 0.3215]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79d1a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_1.state_dict(), './softmaxmodel_ce.PT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9bf934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load do this:\n",
    "model = torch.load('./softmaxmodel_ce.PT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf94a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('LeNet.0.0.weight',\n",
       "              tensor([[[[-0.1343, -0.0068,  0.0198],\n",
       "                        [-0.1361,  0.0117,  0.1259],\n",
       "                        [ 0.0837, -0.1172, -0.1514]],\n",
       "              \n",
       "                       [[-0.1624,  0.1657, -0.0845],\n",
       "                        [-0.1871, -0.1246, -0.0627],\n",
       "                        [-0.0899, -0.1158, -0.1532]],\n",
       "              \n",
       "                       [[-0.1476, -0.1400,  0.1144],\n",
       "                        [ 0.0564, -0.0901, -0.0535],\n",
       "                        [-0.1150,  0.1545, -0.0719]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0372,  0.0714, -0.1404],\n",
       "                        [ 0.0102,  0.0635,  0.1555],\n",
       "                        [ 0.0948, -0.1285, -0.0103]],\n",
       "              \n",
       "                       [[-0.0273,  0.0849,  0.0087],\n",
       "                        [ 0.1437,  0.0448,  0.0980],\n",
       "                        [-0.1116, -0.0326, -0.0358]],\n",
       "              \n",
       "                       [[ 0.1237, -0.0710,  0.0915],\n",
       "                        [ 0.0214,  0.1267,  0.1644],\n",
       "                        [ 0.0545, -0.1927, -0.1920]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0829,  0.1073,  0.1073],\n",
       "                        [-0.0856,  0.0897,  0.0849],\n",
       "                        [-0.0452,  0.0114, -0.0030]],\n",
       "              \n",
       "                       [[-0.1857,  0.0260, -0.1539],\n",
       "                        [ 0.0237,  0.1737,  0.1832],\n",
       "                        [ 0.0429,  0.0125, -0.1237]],\n",
       "              \n",
       "                       [[ 0.0183, -0.1825,  0.0858],\n",
       "                        [ 0.0041,  0.1262, -0.0410],\n",
       "                        [ 0.1381,  0.1654,  0.0750]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0349, -0.1216, -0.1627],\n",
       "                        [ 0.0761, -0.0109,  0.1488],\n",
       "                        [ 0.0849, -0.1641, -0.1076]],\n",
       "              \n",
       "                       [[ 0.0085,  0.1252, -0.1850],\n",
       "                        [-0.0794,  0.0427,  0.0380],\n",
       "                        [-0.1718,  0.1688, -0.0681]],\n",
       "              \n",
       "                       [[ 0.2270, -0.1017,  0.0007],\n",
       "                        [ 0.1450, -0.0847, -0.2070],\n",
       "                        [ 0.1574,  0.1047, -0.0957]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0225,  0.0153, -0.0578],\n",
       "                        [ 0.0173, -0.1343, -0.0271],\n",
       "                        [ 0.1846,  0.1808, -0.0390]],\n",
       "              \n",
       "                       [[-0.1327, -0.1901, -0.0340],\n",
       "                        [-0.0750,  0.1350,  0.0360],\n",
       "                        [-0.0681,  0.0834,  0.2338]],\n",
       "              \n",
       "                       [[-0.2203, -0.2037, -0.0098],\n",
       "                        [-0.0126,  0.1329,  0.0808],\n",
       "                        [-0.0305,  0.0081,  0.1297]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1416,  0.1487, -0.1589],\n",
       "                        [ 0.1941,  0.1898,  0.1649],\n",
       "                        [-0.0882, -0.1479,  0.0743]],\n",
       "              \n",
       "                       [[-0.1237,  0.1441,  0.1483],\n",
       "                        [-0.1242, -0.1114, -0.1893],\n",
       "                        [ 0.0465,  0.1222,  0.0030]],\n",
       "              \n",
       "                       [[-0.1298, -0.0573,  0.1964],\n",
       "                        [-0.1201, -0.1268,  0.1025],\n",
       "                        [-0.1385, -0.0883, -0.1599]]]])),\n",
       "             ('LeNet.0.0.bias',\n",
       "              tensor([-0.0063, -0.1568,  0.1192, -0.1519, -0.1764,  0.1107])),\n",
       "             ('LeNet.0.1.weight',\n",
       "              tensor([1.0309, 1.0271, 1.0046, 0.9893, 1.0574, 1.0249])),\n",
       "             ('LeNet.0.1.bias',\n",
       "              tensor([ 0.0206, -0.0307, -0.0221,  0.0139,  0.0027,  0.0156])),\n",
       "             ('LeNet.0.1.running_mean',\n",
       "              tensor([-0.6821,  0.0264,  0.4724, -0.2362, -0.1885,  0.0578])),\n",
       "             ('LeNet.0.1.running_var',\n",
       "              tensor([0.1447, 0.0115, 0.0401, 0.0037, 0.0027, 0.0016])),\n",
       "             ('LeNet.0.1.num_batches_tracked', tensor(2911)),\n",
       "             ('LeNet.1.0.weight',\n",
       "              tensor([[[[ 0.0494,  0.1275,  0.1059],\n",
       "                        [ 0.0603, -0.1028,  0.0327],\n",
       "                        [ 0.1262,  0.0416,  0.0421]],\n",
       "              \n",
       "                       [[-0.0465,  0.0375, -0.0339],\n",
       "                        [-0.1018,  0.1318, -0.1306],\n",
       "                        [ 0.0068,  0.0043,  0.0231]],\n",
       "              \n",
       "                       [[-0.1211, -0.0469, -0.0389],\n",
       "                        [-0.1330, -0.0034, -0.0434],\n",
       "                        [ 0.0943, -0.0646,  0.1213]],\n",
       "              \n",
       "                       [[ 0.0794, -0.0123, -0.0237],\n",
       "                        [ 0.1173, -0.1305, -0.0553],\n",
       "                        [-0.1106,  0.0745, -0.0051]],\n",
       "              \n",
       "                       [[-0.0941, -0.0628, -0.0298],\n",
       "                        [-0.1121,  0.0812,  0.0658],\n",
       "                        [-0.0442,  0.0197, -0.1310]],\n",
       "              \n",
       "                       [[ 0.0272,  0.0630,  0.1303],\n",
       "                        [ 0.0032, -0.0890, -0.0328],\n",
       "                        [ 0.0438,  0.1082,  0.0994]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0649, -0.0287, -0.0675],\n",
       "                        [-0.1042, -0.0453, -0.0445],\n",
       "                        [-0.1061, -0.0873, -0.0588]],\n",
       "              \n",
       "                       [[ 0.0270, -0.0041, -0.0961],\n",
       "                        [-0.0052, -0.1186, -0.0261],\n",
       "                        [-0.1097, -0.1258, -0.1239]],\n",
       "              \n",
       "                       [[-0.1061, -0.0460,  0.0054],\n",
       "                        [ 0.0408, -0.0819,  0.0699],\n",
       "                        [-0.0609, -0.0642,  0.0016]],\n",
       "              \n",
       "                       [[ 0.0124,  0.0158, -0.0751],\n",
       "                        [-0.0852, -0.0743, -0.0919],\n",
       "                        [ 0.0574, -0.0223, -0.1127]],\n",
       "              \n",
       "                       [[ 0.0786, -0.0958, -0.0606],\n",
       "                        [-0.0240,  0.0832, -0.0984],\n",
       "                        [ 0.1105,  0.1502,  0.1926]],\n",
       "              \n",
       "                       [[-0.1110,  0.0324, -0.0963],\n",
       "                        [ 0.0517, -0.1103, -0.0183],\n",
       "                        [ 0.0982, -0.1332, -0.0027]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0221, -0.0063, -0.0281],\n",
       "                        [-0.1412,  0.0225, -0.0939],\n",
       "                        [ 0.0652, -0.0582, -0.0069]],\n",
       "              \n",
       "                       [[ 0.0572, -0.0196,  0.0188],\n",
       "                        [-0.0636, -0.0699, -0.0899],\n",
       "                        [ 0.0439,  0.0096,  0.0979]],\n",
       "              \n",
       "                       [[-0.0246,  0.1104, -0.0435],\n",
       "                        [-0.0249, -0.0859,  0.1279],\n",
       "                        [-0.0915, -0.0779, -0.0630]],\n",
       "              \n",
       "                       [[ 0.1844, -0.0790, -0.0384],\n",
       "                        [ 0.2123,  0.0371, -0.0107],\n",
       "                        [ 0.1751,  0.0528, -0.1063]],\n",
       "              \n",
       "                       [[-0.0124, -0.0187,  0.0240],\n",
       "                        [ 0.0368, -0.0524, -0.0579],\n",
       "                        [ 0.0651, -0.0412, -0.0634]],\n",
       "              \n",
       "                       [[ 0.0855, -0.0025, -0.0353],\n",
       "                        [-0.1062, -0.1078,  0.1233],\n",
       "                        [ 0.0110, -0.0801, -0.0547]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0157,  0.0053,  0.0435],\n",
       "                        [-0.1219,  0.0516,  0.0367],\n",
       "                        [ 0.0435, -0.0874, -0.0584]],\n",
       "              \n",
       "                       [[-0.0745,  0.0163,  0.1043],\n",
       "                        [ 0.1122,  0.1051, -0.0902],\n",
       "                        [ 0.0185,  0.1248,  0.0559]],\n",
       "              \n",
       "                       [[ 0.1177,  0.0615,  0.0073],\n",
       "                        [-0.1145,  0.0305, -0.1046],\n",
       "                        [ 0.0109, -0.1202, -0.0096]],\n",
       "              \n",
       "                       [[-0.0604, -0.0005,  0.0933],\n",
       "                        [ 0.0641,  0.1168, -0.0889],\n",
       "                        [-0.0727, -0.1046,  0.0385]],\n",
       "              \n",
       "                       [[-0.1272, -0.0933,  0.0604],\n",
       "                        [ 0.0429, -0.0083, -0.0989],\n",
       "                        [-0.0437, -0.0739,  0.0380]],\n",
       "              \n",
       "                       [[-0.1281, -0.0242, -0.0110],\n",
       "                        [ 0.0261, -0.0129,  0.0924],\n",
       "                        [-0.0300,  0.0881, -0.0120]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0031, -0.0207,  0.0838],\n",
       "                        [ 0.0974,  0.1077,  0.1148],\n",
       "                        [ 0.0337, -0.0759, -0.1123]],\n",
       "              \n",
       "                       [[ 0.0202, -0.1087,  0.0976],\n",
       "                        [-0.1307, -0.1366,  0.0634],\n",
       "                        [ 0.0445,  0.0300,  0.0714]],\n",
       "              \n",
       "                       [[ 0.0808,  0.0592,  0.0253],\n",
       "                        [-0.0501, -0.1254, -0.0819],\n",
       "                        [-0.0917,  0.0695,  0.0477]],\n",
       "              \n",
       "                       [[ 0.1218, -0.0330,  0.1416],\n",
       "                        [-0.0324, -0.0486,  0.1172],\n",
       "                        [ 0.0625,  0.1372,  0.0864]],\n",
       "              \n",
       "                       [[-0.0767, -0.1184,  0.0938],\n",
       "                        [-0.0882, -0.1143, -0.1360],\n",
       "                        [-0.0669, -0.1310,  0.0477]],\n",
       "              \n",
       "                       [[ 0.0161,  0.0286, -0.0568],\n",
       "                        [ 0.0608, -0.1314,  0.0483],\n",
       "                        [ 0.0399,  0.0330, -0.1450]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0610,  0.0286,  0.0876],\n",
       "                        [ 0.0697, -0.0973, -0.0179],\n",
       "                        [-0.1010, -0.0126,  0.0161]],\n",
       "              \n",
       "                       [[-0.0043,  0.0383, -0.0519],\n",
       "                        [ 0.0787, -0.0844, -0.0524],\n",
       "                        [-0.0187, -0.0527,  0.0438]],\n",
       "              \n",
       "                       [[-0.0721, -0.0336, -0.1065],\n",
       "                        [-0.0791,  0.0825, -0.0862],\n",
       "                        [ 0.1204,  0.1022, -0.0755]],\n",
       "              \n",
       "                       [[-0.0019,  0.0025,  0.0815],\n",
       "                        [-0.0168, -0.0901, -0.1219],\n",
       "                        [ 0.0454, -0.1060,  0.0618]],\n",
       "              \n",
       "                       [[ 0.2651,  0.2422,  0.0573],\n",
       "                        [ 0.0152,  0.0270, -0.0403],\n",
       "                        [-0.0868,  0.0063, -0.0880]],\n",
       "              \n",
       "                       [[ 0.0184, -0.0891,  0.0374],\n",
       "                        [-0.0692, -0.1187, -0.0178],\n",
       "                        [-0.0489,  0.0464, -0.0869]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0657,  0.1112, -0.0491],\n",
       "                        [-0.1138, -0.1135,  0.1332],\n",
       "                        [-0.0455, -0.1093, -0.1193]],\n",
       "              \n",
       "                       [[ 0.0343, -0.1607, -0.1178],\n",
       "                        [-0.0305, -0.0152,  0.0452],\n",
       "                        [-0.0503, -0.0465,  0.1231]],\n",
       "              \n",
       "                       [[ 0.0850,  0.0130,  0.0085],\n",
       "                        [ 0.0591, -0.0377,  0.0686],\n",
       "                        [-0.1098,  0.1057,  0.0921]],\n",
       "              \n",
       "                       [[-0.0025, -0.0586, -0.0826],\n",
       "                        [-0.0666, -0.0152, -0.0822],\n",
       "                        [-0.0794,  0.0484,  0.0986]],\n",
       "              \n",
       "                       [[-0.1019, -0.0570, -0.1001],\n",
       "                        [-0.0158, -0.0951, -0.1025],\n",
       "                        [-0.0923, -0.0271,  0.0298]],\n",
       "              \n",
       "                       [[-0.0422,  0.0078, -0.0094],\n",
       "                        [-0.1369, -0.0567, -0.0131],\n",
       "                        [ 0.0388,  0.0624, -0.0117]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1186,  0.0346,  0.0613],\n",
       "                        [-0.0935, -0.0153, -0.0257],\n",
       "                        [-0.0951,  0.0238, -0.0439]],\n",
       "              \n",
       "                       [[ 0.0491,  0.0065, -0.1220],\n",
       "                        [-0.0360, -0.0400, -0.0362],\n",
       "                        [-0.1072, -0.0240, -0.0106]],\n",
       "              \n",
       "                       [[-0.0126,  0.0693, -0.0598],\n",
       "                        [ 0.1037, -0.0151, -0.0525],\n",
       "                        [ 0.0572, -0.0678,  0.0488]],\n",
       "              \n",
       "                       [[ 0.1081, -0.0042, -0.0393],\n",
       "                        [-0.1371,  0.1179, -0.0406],\n",
       "                        [ 0.0810,  0.1781,  0.0518]],\n",
       "              \n",
       "                       [[-0.0978,  0.0352, -0.0668],\n",
       "                        [ 0.0316, -0.0615, -0.0935],\n",
       "                        [ 0.0595,  0.0677, -0.1097]],\n",
       "              \n",
       "                       [[ 0.0324,  0.0075, -0.0242],\n",
       "                        [ 0.0601,  0.0633, -0.1143],\n",
       "                        [ 0.0659, -0.0560, -0.1107]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0884, -0.1134, -0.0624],\n",
       "                        [ 0.0405, -0.0264, -0.0104],\n",
       "                        [-0.1157, -0.0633, -0.0324]],\n",
       "              \n",
       "                       [[-0.0393, -0.0812,  0.0075],\n",
       "                        [ 0.0638,  0.0100, -0.0474],\n",
       "                        [-0.0171, -0.0077,  0.0008]],\n",
       "              \n",
       "                       [[-0.0546,  0.0733, -0.0425],\n",
       "                        [ 0.0766, -0.0620,  0.0922],\n",
       "                        [-0.0168, -0.0542,  0.1054]],\n",
       "              \n",
       "                       [[ 0.0206,  0.2006, -0.0803],\n",
       "                        [-0.0692, -0.0676,  0.1692],\n",
       "                        [ 0.1354,  0.0272,  0.1703]],\n",
       "              \n",
       "                       [[ 0.0771, -0.0447, -0.0166],\n",
       "                        [ 0.0943, -0.0103, -0.0471],\n",
       "                        [ 0.0794, -0.0070,  0.0020]],\n",
       "              \n",
       "                       [[ 0.0307, -0.0996,  0.0412],\n",
       "                        [-0.0189,  0.0297, -0.0008],\n",
       "                        [ 0.0720,  0.0073,  0.0816]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0395, -0.0285,  0.1120],\n",
       "                        [-0.0283, -0.0066,  0.0625],\n",
       "                        [-0.0531, -0.1044,  0.0227]],\n",
       "              \n",
       "                       [[-0.0003,  0.0917, -0.0971],\n",
       "                        [ 0.0110, -0.0504,  0.0112],\n",
       "                        [-0.0971, -0.0302, -0.0758]],\n",
       "              \n",
       "                       [[-0.0168,  0.0315, -0.0413],\n",
       "                        [ 0.0187,  0.1041, -0.0717],\n",
       "                        [-0.0776,  0.1225,  0.0732]],\n",
       "              \n",
       "                       [[ 0.1181, -0.0232,  0.0408],\n",
       "                        [-0.0391, -0.1202,  0.1718],\n",
       "                        [-0.0242,  0.1033,  0.1604]],\n",
       "              \n",
       "                       [[-0.1058,  0.0798, -0.0994],\n",
       "                        [-0.1185,  0.0349,  0.0446],\n",
       "                        [-0.0541, -0.0041, -0.0571]],\n",
       "              \n",
       "                       [[-0.0013,  0.1154, -0.0671],\n",
       "                        [-0.0820,  0.0702, -0.1341],\n",
       "                        [-0.0636,  0.0687,  0.0551]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1277, -0.0501,  0.0670],\n",
       "                        [-0.1136,  0.0387,  0.0517],\n",
       "                        [ 0.0538,  0.0982,  0.0059]],\n",
       "              \n",
       "                       [[ 0.0133,  0.0940,  0.1328],\n",
       "                        [-0.1034, -0.0930,  0.0877],\n",
       "                        [ 0.0213,  0.0097, -0.0668]],\n",
       "              \n",
       "                       [[-0.0938,  0.0884, -0.1026],\n",
       "                        [ 0.0227,  0.0050,  0.0497],\n",
       "                        [-0.0116,  0.0792, -0.0360]],\n",
       "              \n",
       "                       [[-0.1503, -0.0422, -0.0022],\n",
       "                        [ 0.0303, -0.0963, -0.0451],\n",
       "                        [ 0.0285,  0.0011, -0.0795]],\n",
       "              \n",
       "                       [[-0.1011,  0.1029, -0.0513],\n",
       "                        [-0.0198, -0.0123, -0.0129],\n",
       "                        [-0.0719,  0.0451, -0.0017]],\n",
       "              \n",
       "                       [[ 0.2301,  0.1114, -0.0154],\n",
       "                        [ 0.1516,  0.1505,  0.0721],\n",
       "                        [-0.0184, -0.0043,  0.0972]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0468,  0.0091, -0.0634],\n",
       "                        [-0.0779,  0.0718, -0.0973],\n",
       "                        [-0.0978,  0.0520,  0.0096]],\n",
       "              \n",
       "                       [[-0.1137,  0.1456,  0.0437],\n",
       "                        [ 0.0211,  0.0497,  0.1320],\n",
       "                        [ 0.0781, -0.1304, -0.0987]],\n",
       "              \n",
       "                       [[-0.0120, -0.1466,  0.0660],\n",
       "                        [-0.0800,  0.0378, -0.0281],\n",
       "                        [ 0.0207,  0.0466,  0.0330]],\n",
       "              \n",
       "                       [[ 0.0284,  0.0146,  0.0235],\n",
       "                        [-0.0074,  0.0567, -0.1012],\n",
       "                        [-0.0492, -0.0266,  0.0295]],\n",
       "              \n",
       "                       [[-0.0637, -0.0612, -0.0698],\n",
       "                        [-0.0256, -0.0131, -0.0572],\n",
       "                        [-0.0004, -0.0022, -0.0716]],\n",
       "              \n",
       "                       [[-0.0290, -0.0484,  0.0613],\n",
       "                        [-0.0362,  0.0318, -0.0541],\n",
       "                        [-0.0464, -0.0237, -0.0062]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0745, -0.0708,  0.0380],\n",
       "                        [-0.1021, -0.0064, -0.0600],\n",
       "                        [-0.0986,  0.0197, -0.0809]],\n",
       "              \n",
       "                       [[-0.0284,  0.0942, -0.1363],\n",
       "                        [ 0.0820, -0.0048,  0.0793],\n",
       "                        [ 0.0819,  0.0255,  0.0572]],\n",
       "              \n",
       "                       [[-0.0206, -0.0078, -0.0237],\n",
       "                        [-0.0893,  0.0145,  0.0477],\n",
       "                        [ 0.1452,  0.0155,  0.1319]],\n",
       "              \n",
       "                       [[ 0.0969, -0.1040, -0.1105],\n",
       "                        [ 0.0280, -0.0551, -0.1196],\n",
       "                        [-0.0459, -0.0197, -0.0689]],\n",
       "              \n",
       "                       [[-0.1403,  0.0931,  0.0383],\n",
       "                        [ 0.0568,  0.0282,  0.0693],\n",
       "                        [-0.0657, -0.1299,  0.0472]],\n",
       "              \n",
       "                       [[ 0.1184, -0.0249, -0.1262],\n",
       "                        [ 0.0785, -0.0198, -0.0305],\n",
       "                        [ 0.0800,  0.0909, -0.1275]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0099, -0.0300,  0.0579],\n",
       "                        [ 0.1015, -0.0747,  0.0477],\n",
       "                        [-0.0976,  0.0296,  0.0873]],\n",
       "              \n",
       "                       [[ 0.0554,  0.0090,  0.0811],\n",
       "                        [-0.0555,  0.0765, -0.0363],\n",
       "                        [-0.0749,  0.0498,  0.0512]],\n",
       "              \n",
       "                       [[ 0.1097,  0.0049, -0.0543],\n",
       "                        [ 0.1153,  0.0922, -0.1097],\n",
       "                        [-0.0924, -0.0469, -0.0939]],\n",
       "              \n",
       "                       [[-0.0104,  0.0839, -0.0293],\n",
       "                        [-0.0011, -0.0308, -0.0235],\n",
       "                        [-0.0068, -0.0866,  0.0055]],\n",
       "              \n",
       "                       [[ 0.0090, -0.0243,  0.0457],\n",
       "                        [-0.0908,  0.1992,  0.2261],\n",
       "                        [-0.0608, -0.0239,  0.1170]],\n",
       "              \n",
       "                       [[ 0.0186,  0.0240,  0.1034],\n",
       "                        [ 0.0300, -0.1211, -0.0547],\n",
       "                        [-0.0705,  0.1379,  0.0531]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1402,  0.0780,  0.0770],\n",
       "                        [-0.1038, -0.0838,  0.1402],\n",
       "                        [ 0.0343, -0.0573, -0.0687]],\n",
       "              \n",
       "                       [[-0.0078,  0.0897,  0.0010],\n",
       "                        [-0.0698, -0.0366, -0.0267],\n",
       "                        [-0.0658, -0.0264, -0.0498]],\n",
       "              \n",
       "                       [[-0.0122, -0.0887, -0.0325],\n",
       "                        [-0.1073,  0.1127, -0.1209],\n",
       "                        [-0.1108, -0.1236, -0.0520]],\n",
       "              \n",
       "                       [[ 0.0385, -0.1238,  0.0403],\n",
       "                        [-0.0873,  0.0469, -0.1400],\n",
       "                        [-0.0957, -0.1386,  0.0053]],\n",
       "              \n",
       "                       [[-0.0777,  0.0456,  0.0115],\n",
       "                        [ 0.0484, -0.1108,  0.0671],\n",
       "                        [-0.0316, -0.1301,  0.0141]],\n",
       "              \n",
       "                       [[-0.0767,  0.0183, -0.0792],\n",
       "                        [-0.1178, -0.0672,  0.0515],\n",
       "                        [-0.1015, -0.0311,  0.0603]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0289,  0.0007,  0.1258],\n",
       "                        [ 0.0360, -0.0176, -0.0786],\n",
       "                        [ 0.0427,  0.0780,  0.0717]],\n",
       "              \n",
       "                       [[-0.0401, -0.0409, -0.0162],\n",
       "                        [-0.0288,  0.0466, -0.0881],\n",
       "                        [ 0.0767,  0.1174, -0.0566]],\n",
       "              \n",
       "                       [[-0.0368,  0.1008, -0.0377],\n",
       "                        [ 0.0741,  0.0144,  0.0541],\n",
       "                        [-0.1207, -0.0278, -0.0405]],\n",
       "              \n",
       "                       [[-0.0997, -0.0553, -0.0348],\n",
       "                        [-0.0981,  0.0104, -0.0304],\n",
       "                        [ 0.0060, -0.1181,  0.0536]],\n",
       "              \n",
       "                       [[-0.0926, -0.0073, -0.0401],\n",
       "                        [ 0.0792, -0.0352, -0.0985],\n",
       "                        [-0.1073, -0.0075,  0.0305]],\n",
       "              \n",
       "                       [[-0.0623, -0.0417,  0.0376],\n",
       "                        [ 0.0328, -0.0645, -0.1202],\n",
       "                        [ 0.2157,  0.1589,  0.0503]]]])),\n",
       "             ('LeNet.1.0.bias',\n",
       "              tensor([-0.0551, -0.0715, -0.1228, -0.1259, -0.1356, -0.0072,  0.1145, -0.1200,\n",
       "                       0.0921,  0.0819,  0.0550,  0.0682,  0.0763, -0.0686,  0.1244,  0.1006])),\n",
       "             ('LeNet.1.1.weight',\n",
       "              tensor([1.0065, 1.0827, 1.0888, 1.0714, 1.0250, 1.1491, 1.1108, 1.0062, 1.0872,\n",
       "                      1.0296, 1.1935, 1.0928, 1.0202, 1.0929, 1.0030, 1.0914])),\n",
       "             ('LeNet.1.1.bias',\n",
       "              tensor([-0.0140,  0.0056,  0.0024, -0.0016, -0.0084, -0.0016,  0.0270, -0.0224,\n",
       "                      -0.0061, -0.0076,  0.0365,  0.0123, -0.0089, -0.0013, -0.0168,  0.0049])),\n",
       "             ('LeNet.1.1.running_mean',\n",
       "              tensor([-0.0327, -0.6701, -0.2137, -0.1745, -0.1507, -0.1094, -0.2959, -0.2944,\n",
       "                       0.2565,  0.0515,  0.1909, -0.1627, -0.0131,  0.1320, -0.4015,  0.0280])),\n",
       "             ('LeNet.1.1.running_var',\n",
       "              tensor([0.0688, 0.0265, 0.0197, 0.0200, 0.0476, 0.0282, 0.0381, 0.0156, 0.0144,\n",
       "                      0.0206, 0.0326, 0.0169, 0.0853, 0.0168, 0.0202, 0.0196])),\n",
       "             ('LeNet.1.1.num_batches_tracked', tensor(2911)),\n",
       "             ('LeNet.3.weight',\n",
       "              tensor([[-0.0018,  0.0075,  0.0116,  ...,  0.0185,  0.0184,  0.0225],\n",
       "                      [-0.0039, -0.0023, -0.0183,  ..., -0.0206, -0.0257, -0.0256]])),\n",
       "             ('LeNet.3.bias', tensor([-0.0075,  0.0053]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5115cdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.42320847511291504\n",
      "Test Acc: 0.875\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "\n",
    "model_1.eval()\n",
    "running_test_loss = 0.0\n",
    "running_test_matched = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        outputs = model_1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # keep track of the loss\n",
    "        running_test_loss += loss.item()\n",
    "\n",
    "        # CALCULATE ACCURACY METRIC\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_test_matched += torch.sum(preds == labels.data)\n",
    "\n",
    "avg_test_loss = running_test_loss\n",
    "avg_test_acc = running_test_matched.double() / len(test_dataloader.dataset)\n",
    "\n",
    "# Print epoch summary\n",
    "print(\"Test Loss: {}\".format(avg_test_loss))\n",
    "print(\"Test Acc: {}\".format(avg_test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
